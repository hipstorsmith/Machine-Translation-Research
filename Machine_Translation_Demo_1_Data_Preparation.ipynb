{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Machine Translation Demo 1. Data Preparation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOL5hJsdsismYhnKYeR9Dpi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hipstorsmith/Machine-Translation-Research/blob/main/Machine_Translation_Demo_1_Data_Preparation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JmgjPBtL5Aw7"
      },
      "source": [
        "import string\n",
        "import re\n",
        "import pickle as pkl\n",
        "from unicodedata import normalize\n",
        "import numpy as np\n",
        "import os"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sFOaGmyw2Xuh"
      },
      "source": [
        "# Preparing the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yUJR75Ca2m6_"
      },
      "source": [
        "## Load data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LrBk09Zg1CO-"
      },
      "source": [
        "def load_doc(filename):\n",
        "  # Open the file as read only\n",
        "  with open(filename, mode='rt', encoding='utf-8') as file:\n",
        "    # Read all data\n",
        "    text = file.read()\n",
        "  return text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bGPn1yj32poX"
      },
      "source": [
        "## Split data into pairs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5JoK9WuX2H37"
      },
      "source": [
        "def to_pairs(doc):\n",
        "\tlines = doc.strip().split('\\n')\n",
        "\tpairs = [line.split('\\t')[:2] for line in  lines]\n",
        "\treturn pairs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xFSmBwx821ev"
      },
      "source": [
        "## Clean data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WerZD-qw2tQ9"
      },
      "source": [
        "def clean_pairs(lines, encoding='unicode', remove_punct=True, remove_nums=False, to_lower=True, unicode_norm=False):\n",
        "  cleaned = list()\n",
        "\n",
        "  # Reg. expression for removing non-printable characters\n",
        "  if encoding != 'unicode':\n",
        "    re_print = re.compile('[^%s]' % re.escape(string.printable))\n",
        "  else:\n",
        "    control_chars = ''.join(map(chr, [int(i) for i in range(0,32)] + [int(i) for i in range(127,160)]))\n",
        "    re_print = re.compile('[%s]' % re.escape(control_chars))\n",
        "\n",
        "  # Translation table for removing punctuation\n",
        "  table = str.maketrans('', '', string.punctuation)\n",
        "  for pair in lines:\n",
        "    clean_pair = []\n",
        "    for line in pair:\n",
        "      # Normalize unicode characters\n",
        "      if unicode_norm:\n",
        "        line = normalize('NFD', line).encode('ascii', 'ignore')\n",
        "        line = line.decode('UTF-8')\n",
        "      \n",
        "      # Split sentence to tokens on white space\n",
        "      line = line.split()\n",
        "\n",
        "      # Convert to lowercase (if needed)\n",
        "      if to_lower:\n",
        "        line = [word.lower() for word in line]\n",
        "\n",
        "      # Remove punctuation from each token (if needed)\n",
        "      if remove_punct:\n",
        "        line = [word.translate(table) for word in line]\n",
        "\n",
        "      # Remove non-printable chars form each token\n",
        "      line = [re_print.sub('', w) for w in line]\n",
        "      \n",
        "      # Remove tokens with numbers in them (if needed)\n",
        "      if remove_nums:\n",
        "        line = [word for word in line if word.isalpha()]\n",
        "\n",
        "\t\t\t# Store cleaned sentence as string\n",
        "      clean_pair.append(' '.join(line))\n",
        "    cleaned.append(clean_pair)\n",
        "  return np.array(cleaned)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qSBGJP8y65xJ"
      },
      "source": [
        "## Save cleaned data to pickle file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L3U0I_Ol7Hwo"
      },
      "source": [
        "def save_clean_data(sentences, filename):\n",
        "\tpkl.dump(sentences, open(filename, 'wb'))\n",
        "\tprint('Saved: %s' % filename)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YZu-gz8C7jxm"
      },
      "source": [
        "## Running all the procedures from above"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J1mtLn2B_33h",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        },
        "outputId": "50501b76-36f6-4083-c4d7-c27407f30e70"
      },
      "source": [
        "# Imports for colab\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "dirname = '/content/gdrive/My Drive/Colab Notebooks/'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JoVYsP9F7kJW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 605
        },
        "outputId": "b0444444-7761-4b4a-c468-dbffa9e0fb34"
      },
      "source": [
        "# Load dataset\n",
        "filename = dirname + 'rus.txt'\n",
        "doc = load_doc(filename)\n",
        "\n",
        "# Split into english-russian pairs\n",
        "pairs = to_pairs(doc)\n",
        "\n",
        "# Clean sentences\n",
        "cleaned_pairs = clean_pairs(pairs)\n",
        "\n",
        "# Save cleaned pairs to file\n",
        "save_clean_data(cleaned_pairs, dirname + 'english-russian.pkl')\n",
        "\n",
        "# Checking for data\n",
        "print('Data shape: ', cleaned_pairs.shape)\n",
        "\n",
        "for i in range(30):\n",
        "\tprint('[%s] => [%s]' % (cleaned_pairs[i,0], cleaned_pairs[i,1]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saved: /content/gdrive/My Drive/Colab Notebooks/english-russian.pkl\n",
            "Data shape:  (370085, 2)\n",
            "[go] => [марш]\n",
            "[go] => [иди]\n",
            "[go] => [идите]\n",
            "[hi] => [здравствуйте]\n",
            "[hi] => [привет]\n",
            "[hi] => [хай]\n",
            "[hi] => [здрасте]\n",
            "[hi] => [здоро́во]\n",
            "[run] => [беги]\n",
            "[run] => [бегите]\n",
            "[run] => [беги]\n",
            "[run] => [бегите]\n",
            "[who] => [кто]\n",
            "[wow] => [вот это да]\n",
            "[wow] => [круто]\n",
            "[wow] => [здорово]\n",
            "[wow] => [ух ты]\n",
            "[wow] => [ого]\n",
            "[wow] => [вах]\n",
            "[fire] => [огонь]\n",
            "[fire] => [пожар]\n",
            "[help] => [помогите]\n",
            "[help] => [на помощь]\n",
            "[help] => [спасите]\n",
            "[jump] => [прыгай]\n",
            "[jump] => [прыгайте]\n",
            "[jump] => [прыгай]\n",
            "[jump] => [прыгайте]\n",
            "[stop] => [стой]\n",
            "[stop] => [остановитесь]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h5d25wYcGjg3"
      },
      "source": [
        "# Shuffling, reduction and splitting the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tA9o0S8FHGks"
      },
      "source": [
        "## Load clean data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1mGoX8IC_bWs"
      },
      "source": [
        "def load_clean_sentences(filename):\n",
        "  return pkl.load(open(filename, 'rb'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4x9IaQooHYK6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "fb3b4aa8-492a-472d-9115-54e9702b4d97"
      },
      "source": [
        "raw_dataset = load_clean_sentences(dirname + 'english-russian.pkl')\n",
        "\n",
        "# Reduce dataset size (if needed)\n",
        "reduction = True\n",
        "n_sentences = 30000\n",
        "if reduction:\n",
        "  idx = str(n_sentences//1000)+'k'\n",
        "  dataset = raw_dataset[:n_sentences, :]\n",
        "else:\n",
        "  idx = 'full'\n",
        "  dataset = raw_dataset\n",
        "\n",
        "# Random shuffle\n",
        "np.random.seed(0)\n",
        "np.random.shuffle(dataset)\n",
        "\n",
        "# Split into train/test\n",
        "train_share = 0.9\n",
        "train_len = int(dataset.shape[0]*train_share)\n",
        "\n",
        "train, test = dataset[:train_len], dataset[train_len:]\n",
        "# Uncomment next three lines and comment previous one, if you want train-val-test split\n",
        "# Also don't forget to change val_share, if you need and dump val set to pickle\n",
        "#val_share = 0.05\n",
        "#val_len = int(dataset.shape[0]*val_share)\n",
        "#train, val, test = dataset[:train_len], dataset[train_len:train_len + val_len], dataset[train_len + val_len:]\n",
        "\n",
        "# Save\n",
        "save_clean_data(dataset, dirname + 'english-russian-' + idx +'-both.pkl')\n",
        "save_clean_data(train, dirname + 'english-russian-' + idx + '-train.pkl')\n",
        "save_clean_data(test, dirname + 'english-russian-'+ idx + '-test.pkl')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saved: /content/gdrive/My Drive/Colab Notebooks/english-russian-30k-both.pkl\n",
            "Saved: /content/gdrive/My Drive/Colab Notebooks/english-russian-30k-train.pkl\n",
            "Saved: /content/gdrive/My Drive/Colab Notebooks/english-russian-30k-test.pkl\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4vF7ND74JCtD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "outputId": "a2d5ac66-ea10-49dc-d8f7-b712262cb8d4"
      },
      "source": [
        "for i in range(10):\n",
        "  print('Train: [%s] => [%s]' % (train[i,0], train[i,1]))\n",
        "  print('Test: [%s] => [%s]' % (test[i,0], test[i,1]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train: [i buried it] => [я её закопал]\n",
            "Test: [go brush your teeth] => [идите почистите зубы]\n",
            "Train: [go and wake up mary] => [пойди разбуди мэри]\n",
            "Test: [tom has been told] => [тому сказали]\n",
            "Train: [she did a good job] => [она проделала хорошую работу]\n",
            "Test: [tom has found mary] => [том нашёл мэри]\n",
            "Train: [i work at a zoo] => [я работаю в зоопарке]\n",
            "Test: [it must be a mistake] => [это наверное ошибка]\n",
            "Train: [i want them] => [я хочу их]\n",
            "Test: [this is a bad idea] => [это плохая идея]\n",
            "Train: [ive been thinking] => [я размышляю]\n",
            "Test: [tom bent down] => [том нагнулся]\n",
            "Train: [we talked about boys] => [мы говорили о мальчиках]\n",
            "Test: [its as plain as day] => [это ясно как день]\n",
            "Train: [is that blood] => [это кровь]\n",
            "Test: [i was not drunk] => [я не была пьяна]\n",
            "Train: [dont be so selfish] => [не будь такой эгоисткой]\n",
            "Test: [can we rest a while] => [мы можем немного передохнуть]\n",
            "Train: [people are stupid] => [люди глупы]\n",
            "Test: [are you a racist] => [вы расист]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gKqMzNrrKKY1"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}